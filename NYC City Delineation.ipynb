{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City district delineation using Graph Neural Networks\n",
    "\n",
    "### Graph Neural Networks\n",
    "\n",
    "\n",
    "Graph Neural Networks (GNNs) are a class of deep learning methods designed to perform inference on data described by graphs. GNNs are neural networks that can be directly applied to graphs, and provide an easy way to do node-level, edge-level, and graph-level prediction tasks. \n",
    "\n",
    "Anything that is composed of linked entities can be represented as a graph. Graphs are excellent tools to visualize relations between people, objects, and concepts. With graphs becoming more pervasive and richer with information, and artificial neural networks becoming more popular and capable, GNNs have become a powerful tool for many important applications.\n",
    "\n",
    "<img src='GNN.png' width=\"600\" height=\"600\">\n",
    "\n",
    "Graph neural networks can be created like any other neural network, using fully connected layers, convolutional layers, pooling layers, etc. The type and number of layers depend on the type and complexity of the graph data and the desired output.\n",
    "\n",
    "The GNN receives the formatted graph data as input and produces a vector of numerical values that represent relevant information about nodes and their relations. This vector representation is called “graph embedding.”\n",
    "\n",
    "One very popular GNN architecture is the graph convolutional neural network (GCN), which uses convolution layers to create graph embeddings. (Kipf, T.N. and Welling, M., 2016. Semi-supervised classification with graph convolutional networks. https://arxiv.org/pdf/1609.02907.pdf)\n",
    "\n",
    "In the recent years, a lot of work has been done on the problem of generalizing neural networks to work on arbitrarily structured graphs - Bruna, J., Zaremba, W., Szlam, A. and LeCun, Y., 2013. Spectral networks and locally connected networks on graphs. https://arxiv.org/pdf/1312.6203.pdf%20http://arxiv.org/abs/1312.6203.pdf,  Henaff, M., Bruna, J. and LeCun, Y., 2015. Deep convolutional networks on graph-structured data. https://arxiv.org/abs/1506.05163 etc.\n",
    "\n",
    " \n",
    "Few applications for graph neural networks:\n",
    "\n",
    "- Node classification: One of the powerful applications of GNNs is adding new information to nodes or filling gaps where information is missing. For example, say you are running a social network and you have spotted a few bot accounts. Now you want to find out if there are other bot accounts in your network. You can train a GNN to classify other users in the social network as “bot” or “not bot” based on how close their graph embeddings are to those of the known bots.\n",
    "\n",
    "- Edge prediction: Another way to put GNNs to use is to find new edges that can add value to the graph. Going back to our social network, a GNN can find users (nodes) who are close to you in embedding space but who aren’t your friends yet (i.e., there isn’t an edge connecting you to each other). These users can then be introduced to you as friend suggestions.\n",
    "\n",
    "- Clustering: GNNs can glean new structural information from graphs. For example, in a social network where everyone is in one way or another related to others (through friends, or friends of friends, etc.), the GNN can find nodes that form clusters in the embedding space.\n",
    "\n",
    "A comprehensive tutorial on GNNs by Stanford is available on Youtube: https://www.youtube.com/watch?v=JAB_plj2rbA&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn\n",
    "\n",
    "### GNN for city delineation\n",
    "\n",
    "Here, we demonstrate an application for GNN for city borough delineation. The relationships among various entities such as interaction among people, intra-city mobility, social media interactions can be interpreted in terms of graphs with many features associated with nodes, edges. \n",
    "\n",
    "We will use the LEHD mobility network among zip codes in NYC and use it to learn the corresponding borough of zip codes (nodes) in the city. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import geopandas as gpd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data overview\n",
    "\n",
    "The LEHD mobility matrix contains the commute numbers between the home and work zip codes of the population..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>weight</th>\n",
       "      <th>initialFeat</th>\n",
       "      <th>true_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11436</td>\n",
       "      <td>10009</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11436</td>\n",
       "      <td>10011</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11436</td>\n",
       "      <td>10013</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11436</td>\n",
       "      <td>10019</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11436</td>\n",
       "      <td>10021</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin  destination  weight  initialFeat  true_label\n",
       "0   11436        10009       1          4.0           4\n",
       "1   11436        10011       1          4.0           4\n",
       "2   11436        10013       1          4.0           4\n",
       "3   11436        10019       1          4.0           4\n",
       "4   11436        10021       1          4.0           4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/zips_merged.csv', delimiter=',')\n",
    "data = data.rename(columns={'total': 'weight', 'w_zip':'origin', 'h_zip':'destination'})\n",
    "data = data[data.destination.isin(data.origin.unique())]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "populationByAge = pd.read_csv('data/zicode_populationByAge.csv', delimiter=',')\n",
    "populationByAge = populationByAge.iloc[:,0:2]\n",
    "populationByAge.rename(columns={'ZIPCODE':'destination'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housePrice = pd.read_csv('data/zipcode_housePrice.csv', delimiter=',')\n",
    "weights = [5000,12500,17500,22500,27500,32500,37500,45000,55000,65000,75000,85000\n",
    "                    ,95000,112500,137500,162500,187500,225000,275000,350000,450000,625000,875000\n",
    "                        ,1250000,1750000,2000000]\n",
    "for i in range(len(weights)):\n",
    "    housePrice.iloc[i,2:] = housePrice.iloc[i,2:]*weights[i]\n",
    "housePrice.iloc[:,1] [housePrice.iloc[:,1] == 0] = 1\n",
    "\n",
    "tmp = (housePrice.iloc[:,2:] != 0 ).sum(axis=1)\n",
    "tmp[tmp == 0] = 1\n",
    "housePrice = pd.concat([housePrice.iloc[:,0],housePrice.iloc[:,2:].sum(axis=1) / tmp ], axis = 1 )\n",
    "housePrice.rename(columns={'ZIPCODE':'destination'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = pd.read_csv('data/zips_area.csv', delimiter=',')\n",
    "area = area.iloc[:,:2]\n",
    "area.rename(columns={'ZIPCODE':'destination'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "income = pd.read_csv('data/zipcode_income.csv', delimiter=',')\n",
    "income = income.iloc[:,:2]\n",
    "income[income.isna()] = 0\n",
    "income.rename(columns={'ZIPCODE':'destination'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "populationJobs = pd.read_csv('data/zipcode_population_Jobs.csv', delimiter=',')\n",
    "populationJobs = populationJobs.iloc[:,:2]\n",
    "populationJobs.rename(columns={'ZIPCODE':'destination'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(populationByAge)\n",
    "data = data.merge(housePrice)\n",
    "data = data.merge(area)\n",
    "data = data.merge(income)\n",
    "data = data.merge(populationJobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>weight</th>\n",
       "      <th>initialFeat</th>\n",
       "      <th>true_label</th>\n",
       "      <th>Estimate!!Total!!Total population</th>\n",
       "      <th>0</th>\n",
       "      <th>AREA</th>\n",
       "      <th>median_familyIncome(USD)</th>\n",
       "      <th>totalJobs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11436</td>\n",
       "      <td>10009</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>33620</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1.590352e+07</td>\n",
       "      <td>60994.34833</td>\n",
       "      <td>8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11213</td>\n",
       "      <td>10009</td>\n",
       "      <td>14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>33620</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1.590352e+07</td>\n",
       "      <td>60994.34833</td>\n",
       "      <td>8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11212</td>\n",
       "      <td>10009</td>\n",
       "      <td>27</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>33620</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1.590352e+07</td>\n",
       "      <td>60994.34833</td>\n",
       "      <td>8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11225</td>\n",
       "      <td>10009</td>\n",
       "      <td>26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>33620</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1.590352e+07</td>\n",
       "      <td>60994.34833</td>\n",
       "      <td>8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11218</td>\n",
       "      <td>10009</td>\n",
       "      <td>60</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>33620</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1.590352e+07</td>\n",
       "      <td>60994.34833</td>\n",
       "      <td>8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36407</th>\n",
       "      <td>11211</td>\n",
       "      <td>11371</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.055847e+07</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36408</th>\n",
       "      <td>11373</td>\n",
       "      <td>11371</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.055847e+07</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36409</th>\n",
       "      <td>10168</td>\n",
       "      <td>11371</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.055847e+07</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36410</th>\n",
       "      <td>10278</td>\n",
       "      <td>11371</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.055847e+07</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36411</th>\n",
       "      <td>10036</td>\n",
       "      <td>11371</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.055847e+07</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36412 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       origin  destination  weight  initialFeat  true_label  \\\n",
       "0       11436        10009       1          4.0           4   \n",
       "1       11213        10009      14          3.0           3   \n",
       "2       11212        10009      27          3.0           3   \n",
       "3       11225        10009      26          3.0           3   \n",
       "4       11218        10009      60          3.0           3   \n",
       "...       ...          ...     ...          ...         ...   \n",
       "36407   11211        11371       7          1.0           3   \n",
       "36408   11373        11371       3          4.0           4   \n",
       "36409   10168        11371       1          1.0           1   \n",
       "36410   10278        11371       1          1.0           1   \n",
       "36411   10036        11371       5          1.0           1   \n",
       "\n",
       "       Estimate!!Total!!Total population          0          AREA  \\\n",
       "0                                  33620  1200000.0  1.590352e+07   \n",
       "1                                  33620  1200000.0  1.590352e+07   \n",
       "2                                  33620  1200000.0  1.590352e+07   \n",
       "3                                  33620  1200000.0  1.590352e+07   \n",
       "4                                  33620  1200000.0  1.590352e+07   \n",
       "...                                  ...        ...           ...   \n",
       "36407                                  0        0.0  3.055847e+07   \n",
       "36408                                  0        0.0  3.055847e+07   \n",
       "36409                                  0        0.0  3.055847e+07   \n",
       "36410                                  0        0.0  3.055847e+07   \n",
       "36411                                  0        0.0  3.055847e+07   \n",
       "\n",
       "       median_familyIncome(USD)  totalJobs  \n",
       "0                   60994.34833       8929  \n",
       "1                   60994.34833       8929  \n",
       "2                   60994.34833       8929  \n",
       "3                   60994.34833       8929  \n",
       "4                   60994.34833       8929  \n",
       "...                         ...        ...  \n",
       "36407                   0.00000      10651  \n",
       "36408                   0.00000      10651  \n",
       "36409                   0.00000      10651  \n",
       "36410                   0.00000      10651  \n",
       "36411                   0.00000      10651  \n",
       "\n",
       "[36412 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'weight': commute flow between origin and destination zips\n",
    "- 'initialFeat': inital borough attachment labels for each zip code, assigned to be the borough with the closest distance between borough and zip centroids\n",
    "- 'true_label': true borough label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial model config\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "weight_decay = 10e-4\n",
    "epochs = 10001\n",
    "seed = 165\n",
    "hidden = 10\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a symmetric normalization for the propogating the layer, i.e. $$D^{−1/2}AD^{−1/2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(adj):\n",
    "\n",
    "    adj = torch.FloatTensor(adj)\n",
    "    adj_id = torch.FloatTensor(torch.eye(adj.shape[1]))\n",
    "    adj_id = adj_id.reshape((1, adj.shape[1], adj.shape[1]))\n",
    "    adj_id = adj_id.repeat(adj.shape[0], 1, 1)\n",
    "    adj = adj + adj_id\n",
    "    rowsum = torch.FloatTensor(adj.sum(2))\n",
    "    degree_mat_inv_sqrt = torch.diag_embed(torch.float_power(rowsum,-0.5), dim1=-2, dim2=-1).float()\n",
    "    adj_norm = torch.bmm(torch.transpose(torch.bmm(adj,degree_mat_inv_sqrt),1,2),degree_mat_inv_sqrt)\n",
    "\n",
    "    return adj_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function for the GNN is the double ReLU which is linear between between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doublerelu(x):\n",
    "    return torch.clamp(x, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Layer propogation rule of the GNN is : $$ H^{(l+1)} = f(H^{(l)}, A) = \\sigma ( H^{(l)}W_1^{(l)} + \\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W_2^{(l)}) $$\n",
    "\n",
    "where H is the l'th neural network layer, A is the adjaceny matrix, D is the diagonal node degree matrix, W1 , W2 are learnable weight matrices initialised as W1 = 1, W2 = 0  and sigma is the activation function doublerelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN1Layer(Module):\n",
    "\n",
    "    def __init__(self, batch_size, in_features, out_features):\n",
    "        super(GNN1Layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Initialse W1 = 1, W2 = 0 as pytorch learnable weights (parameters) that have require_grad = True which is\n",
    "        # required for calculating gradients while backpropogating using gradient descent\n",
    "        weight1_eye = torch.FloatTensor(torch.eye(in_features, out_features))\n",
    "        weight1_eye = weight1_eye.reshape((1, in_features, out_features))\n",
    "        weight1_eye = weight1_eye.repeat(batch_size, 1, 1)\n",
    "        self.weight1 = Parameter(weight1_eye)\n",
    "        self.weight2 = Parameter(torch.zeros(batch_size, in_features, out_features))\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # first term H*W1\n",
    "        v1 = torch.bmm(input, self.weight1)\n",
    "        # second term adj_norm*H*W2\n",
    "        v2 = torch.bmm(torch.bmm(adj, input), self.weight2)\n",
    "        # adding the two terms\n",
    "        output = v1 + v2\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN1(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, nfeat, ndim, hidden):\n",
    "        super(GNN1, self).__init__()\n",
    "\n",
    "        self.gc1 = GNN1Layer(batch_size, nfeat, ndim)\n",
    "\n",
    "    def forward(self, x, adj, random_indices):\n",
    "        f = torch.clone(x)\n",
    "        # Applying activation function sigma (doublerelu) on the layer propogation\n",
    "        x = doublerelu(self.gc1(x, adj))\n",
    "        x = x/x.sum(axis=2).unsqueeze(2) #normalize st sum = 1\n",
    "        \n",
    "        # Only the masked nodes are updated in backpropogation \n",
    "        f[0][random_indices, :x.shape[2]] = x[0][random_indices, :]\n",
    "\n",
    "        return f[:,:,:x.shape[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svdApprox(adj, dim, relu=False):\n",
    "    adj = torch.FloatTensor(adj[0])\n",
    "    U, S, Vh = torch.linalg.svd(adj)\n",
    "    mu = torch.matmul(torch.matmul(U[:, :dim], torch.diag(S[:dim])), Vh[:dim, :])\n",
    "\n",
    "    embedx = torch.matmul(U[:, :dim], torch.diag(torch.pow(S[:dim], 0.5)))\n",
    "    embedy = torch.transpose(torch.matmul(torch.diag(torch.pow(S[:dim], 0.5)), Vh[:dim, :]), 0, 1)\n",
    "\n",
    "    return embedx, embedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(adj,features,labels,random_indices,test_random_indices, test_features):\n",
    "    \n",
    "    # calculate symmetric normalisation for layer propogation\n",
    "    adj_norm = normalize(adj)\n",
    "    \n",
    "    labels = labels - 1\n",
    "    \n",
    "    # Convert from numpy to torch tensors\n",
    "    adj = torch.FloatTensor(adj)\n",
    "    adj_norm = torch.FloatTensor(adj_norm)\n",
    "    features = torch.FloatTensor(features)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "    test_features = torch.FloatTensor(test_features)\n",
    "    \n",
    "    embedx, embedy = svdApprox(adj,dim=3)\n",
    "    embedx = embedx.unsqueeze(0)\n",
    "    embedy = embedy.unsqueeze(0)\n",
    "    embedx = torch.zeros(embedx.shape)\n",
    "    embedy = torch.zeros(embedy.shape)\n",
    "    features = torch.cat((features,embedx,embedy),axis=2)\n",
    "    test_features = torch.cat((test_features,embedx,embedy),axis=2)\n",
    "    \n",
    "    # initialise the mode\n",
    "    model = GNN1(batch_size=adj.shape[0],\n",
    "                nfeat=features.shape[-1],\n",
    "                ndim=nb_label,\n",
    "                hidden=hidden)\n",
    "    # Transfer the weights to GPU for training\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "        features = features.cuda()\n",
    "        adj = adj.cuda()\n",
    "        adj_norm = adj_norm.cuda()\n",
    "        labels = labels.cuda()\n",
    "        test_features = test_features.cuda()\n",
    "    \n",
    "    # Train model\n",
    "    t_total = time.time()\n",
    "\n",
    "    # Using adam optimizers for backpropogation\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # loss function criteria\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train for the no of epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        # Pytorch accumulates gradient after every operation on tensors (defined by the model architecture)\n",
    "        # with require_grad = True. With each new epoch, we need to reset this gradient to 0 to calculate gradient\n",
    "        # for this epoch.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the output from forward propogation of our model\n",
    "        output = model(features, adj_norm, random_indices)\n",
    "            \n",
    "        # Mask accuracy\n",
    "        accuracy = torch.sum(torch.argmax(output[0][random_indices, :],axis=1)==labels[random_indices, :].reshape(1,-1))/labels[random_indices, :].shape[0]\n",
    "        \n",
    "        #Test accuracy.\n",
    "        # Use torch.no_grad() because test set is not used to calculate and update weights using the gradient value.\n",
    "        # No gradients will be calculated within this statement and hence no weights will be updated.\n",
    "        with torch.no_grad():\n",
    "            test_output = model(test_features, adj_norm, test_random_indices)\n",
    "            test_accuracy = torch.sum(torch.argmax(test_output[0][test_random_indices, :],axis=1)==labels[test_random_indices, :].reshape(1,-1))/labels[test_random_indices, :].shape[0]\n",
    "        \n",
    "        # Calculate the loss between our models training output and true label\n",
    "        loss = criterion(output[0],labels.reshape(-1).long())\n",
    "\n",
    "        # Calculate the gradients \n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        # Update the wieghts\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print summary of training \n",
    "        if epoch == 0:\n",
    "            best_loss = loss\n",
    "            best_output = output\n",
    "            best_acc = accuracy\n",
    "            init_acc = accuracy\n",
    "            test_init_acc = accuracy\n",
    "            best_test_acc = test_accuracy\n",
    "            best_test_op = test_output\n",
    "        else:\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_output = output\n",
    "                best_acc = accuracy\n",
    "                best_test_acc = test_accuracy\n",
    "                best_test_op = test_output\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch: {:04d}'.format(epoch + 1),\n",
    "                  'Train Accuracy: {:.4f}'.format(best_acc.item()),\n",
    "                  'Test Accuracy: {:.4f}'.format(best_test_acc.item()),\n",
    "                  'Loss: {:.8f}'.format(best_loss.item()),\n",
    "                  'time: {:.4f}s'.format(time.time() - t))\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    \n",
    "    return best_loss,best_output, init_acc, best_acc, best_test_op, best_test_acc, test_init_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   origin  destination  weight  initialFeat  true_label\n",
      "0   11436        10009       1          4.0           4\n",
      "1   11436        10011       1          4.0           4\n",
      "2   11436        10013       1          4.0           4\n",
      "3   11436        10019       1          4.0           4\n",
      "4   11436        10021       1          4.0           4\n"
     ]
    }
   ],
   "source": [
    "# function for loading data, returns adjacency matrix, initial feature assignments and true labels\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    data = pd.read_csv('data/zips_merged.csv', delimiter=',')\n",
    "    data = data.rename(columns={'total': 'weight', 'w_zip':'origin', 'h_zip':'destination'})\n",
    "    data = data[data.destination.isin(data.origin.unique())]\n",
    "    print(data.head())\n",
    "    G = nx.from_pandas_edgelist(data, 'origin', 'destination', 'weight',create_using=nx.DiGraph())\n",
    "    adj_list = np.array([nx.adjacency_matrix(G).todense()], dtype=float)\n",
    "    init_feat = np.array(data.groupby('origin')['initialFeat'].agg(['unique']))\n",
    "    true_label = np.array(data.groupby('origin')['true_label'].agg(['unique']))\n",
    "    init_feat = np.array(list(map(lambda x: x[0], init_feat))).reshape(-1, 1)\n",
    "    true_label = np.array(list(map(lambda x: x[0][0], true_label))).reshape(-1, 1)\n",
    "    return adj_list,init_feat,true_label\n",
    "\n",
    "adj,feature,labels = load_data()\n",
    "\n",
    "label = np.copy(labels)\n",
    "label = label - 1\n",
    "nb_label = int(max(label))+1\n",
    "featuress = np.eye(nb_label)[np.array(label,dtype=int).reshape(1,-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Masked 30% of nodes\n",
      "\n",
      "Epoch: 0001 Train Accuracy: 0.3725 Test Accuracy: 0.5333 Loss: 1.07759678 time: 0.3855s\n",
      "Epoch: 1001 Train Accuracy: 0.3725 Test Accuracy: 0.5333 Loss: 1.04274487 time: 0.0030s\n",
      "Epoch: 2001 Train Accuracy: 0.6275 Test Accuracy: 0.6667 Loss: 1.02035475 time: 0.0043s\n",
      "Epoch: 3001 Train Accuracy: 0.6275 Test Accuracy: 0.6667 Loss: 1.02035475 time: 0.0015s\n",
      "Epoch: 4001 Train Accuracy: 0.6275 Test Accuracy: 0.6667 Loss: 1.02035475 time: 0.0030s\n",
      "Epoch: 5001 Train Accuracy: 0.6275 Test Accuracy: 0.6667 Loss: 1.02035475 time: 0.0015s\n",
      "Epoch: 6001 Train Accuracy: 0.6275 Test Accuracy: 0.6667 Loss: 1.02035475 time: 0.0030s\n",
      "Epoch: 7001 Train Accuracy: 0.6275 Test Accuracy: 0.6667 Loss: 1.02035475 time: 0.0000s\n",
      "Epoch: 8001 Train Accuracy: 0.6275 Test Accuracy: 0.6667 Loss: 1.02035475 time: 0.0042s\n",
      "Epoch: 9001 Train Accuracy: 0.6275 Test Accuracy: 0.6667 Loss: 1.02035475 time: 0.0010s\n",
      "Epoch: 10001 Train Accuracy: 0.6275 Test Accuracy: 0.6667 Loss: 1.02035475 time: 0.0013s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 31.8780s\n",
      "Epoch: 0001 Train Accuracy: 0.6275 Test Accuracy: 0.6667 Loss: 1.02035475 time: 0.0025s\n",
      "Epoch: 1001 Train Accuracy: 0.6275 Test Accuracy: 0.6667 Loss: 1.00778162 time: 0.0000s\n",
      "Epoch: 2001 Train Accuracy: 0.6078 Test Accuracy: 0.6667 Loss: 0.98668981 time: 0.0022s\n",
      "Epoch: 3001 Train Accuracy: 0.6078 Test Accuracy: 0.6667 Loss: 0.98668981 time: 0.0085s\n",
      "Epoch: 4001 Train Accuracy: 0.6078 Test Accuracy: 0.6667 Loss: 0.98668981 time: 0.0035s\n",
      "Epoch: 5001 Train Accuracy: 0.6078 Test Accuracy: 0.6667 Loss: 0.98668981 time: 0.0111s\n",
      "Epoch: 6001 Train Accuracy: 0.6078 Test Accuracy: 0.6667 Loss: 0.98668981 time: 0.0000s\n",
      "Epoch: 7001 Train Accuracy: 0.6078 Test Accuracy: 0.6667 Loss: 0.98668981 time: 0.0060s\n",
      "Epoch: 8001 Train Accuracy: 0.6078 Test Accuracy: 0.6667 Loss: 0.98668981 time: 0.0000s\n",
      "Epoch: 9001 Train Accuracy: 0.6078 Test Accuracy: 0.6667 Loss: 0.98668981 time: 0.0000s\n",
      "Epoch: 10001 Train Accuracy: 0.6078 Test Accuracy: 0.6667 Loss: 0.98668981 time: 0.0090s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 33.9003s\n",
      "Epoch: 0001 Train Accuracy: 0.6078 Test Accuracy: 0.6667 Loss: 0.98668981 time: 0.0115s\n",
      "Epoch: 1001 Train Accuracy: 0.6275 Test Accuracy: 0.6667 Loss: 0.98301959 time: 0.0070s\n",
      "Epoch: 2001 Train Accuracy: 0.6863 Test Accuracy: 0.6667 Loss: 0.97768569 time: 0.0010s\n",
      "Epoch: 3001 Train Accuracy: 0.7451 Test Accuracy: 0.6667 Loss: 0.97410619 time: 0.0030s\n",
      "Epoch: 4001 Train Accuracy: 0.7451 Test Accuracy: 0.6667 Loss: 0.97410619 time: 0.0076s\n",
      "Epoch: 5001 Train Accuracy: 0.7451 Test Accuracy: 0.6667 Loss: 0.97410619 time: 0.0013s\n",
      "Epoch: 6001 Train Accuracy: 0.7451 Test Accuracy: 0.6667 Loss: 0.97410619 time: 0.0075s\n",
      "Epoch: 7001 Train Accuracy: 0.7451 Test Accuracy: 0.6667 Loss: 0.97410619 time: 0.0000s\n",
      "Epoch: 8001 Train Accuracy: 0.7451 Test Accuracy: 0.6667 Loss: 0.97410619 time: 0.0000s\n",
      "Epoch: 9001 Train Accuracy: 0.7451 Test Accuracy: 0.6667 Loss: 0.97410619 time: 0.0000s\n",
      "Epoch: 10001 Train Accuracy: 0.7451 Test Accuracy: 0.6667 Loss: 0.97410619 time: 0.0075s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 30.9643s\n"
     ]
    }
   ],
   "source": [
    "# set mask %\n",
    "mask_percentage = [0.3]\n",
    "init = []\n",
    "final = []\n",
    "test_init = []\n",
    "test_final = []\n",
    "\n",
    "for m in mask_percentage:\n",
    "    \n",
    "    features = np.copy(featuress)\n",
    "    test_features = np.copy(featuress)\n",
    "    \n",
    "    # Masking for train and test set\n",
    "    number_of_rows = features[0].shape[0]\n",
    "    random_indices = np.random.choice(number_of_rows, size=int(m*number_of_rows), replace=False)\n",
    "    test_random_indices = np.random.choice(number_of_rows, size=int(0.1*number_of_rows), replace=False)\n",
    "    \n",
    "    # exclude low activity areas from masking\n",
    "    zip_sum = data.groupby(by='origin', as_index=False).sum()\n",
    "    low_act = zip_sum[zip_sum.weight < 3000].index\n",
    "    \n",
    "    random_indices = np.setdiff1d(random_indices, low_act)\n",
    "    random_rows = features[0][random_indices, :]\n",
    "    features[0][random_indices, :] = np.tile(np.array([[0.2]]),random_rows.shape)\n",
    "    \n",
    "    test_random_indices = np.setdiff1d(test_random_indices, low_act)\n",
    "    test_random_rows = test_features[0][test_random_indices, :]\n",
    "    test_features[0][test_random_indices, :] = np.tile(np.array([[0.2]]),test_random_rows.shape)\n",
    "    \n",
    "    \n",
    "    print(\"\\nMasked {}% of nodes\\n\".format(int(m*100)))\n",
    "    prev_loss, op, acc, _, test_op, _, test_acc = train(adj,features,labels, random_indices, test_random_indices, test_features)\n",
    "    init.append(acc.item())\n",
    "    test_init.append(test_acc.item())\n",
    "\n",
    "    loss, op, _, acc, test_op, test_acc, _ = train(adj,op.cpu().detach().numpy(),labels, random_indices, test_random_indices, test_op.cpu().detach().numpy())\n",
    "    i = 0\n",
    "    while loss < prev_loss :\n",
    "        i += 1\n",
    "        if i >= 2:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "        loss, op, _, acc, test_op, test_acc, _ = train(adj,op.cpu().detach().numpy(),labels, random_indices, test_random_indices, test_op.cpu().detach().numpy())\n",
    "    final.append(acc.item())\n",
    "    test_final.append(test_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mask Percentage</th>\n",
       "      <th>Train Initial Accuracy</th>\n",
       "      <th>Train Final Accuracy</th>\n",
       "      <th>Test Initial Accuracy</th>\n",
       "      <th>Test Final Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30/10</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Mask Percentage  Train Initial Accuracy  Train Final Accuracy  \\\n",
       "0           30/10                0.372549              0.745098   \n",
       "\n",
       "   Test Initial Accuracy  Test Final Accuracy  \n",
       "0               0.372549             0.666667  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'Mask Percentage': [\"30/10\"], 'Train Initial Accuracy': init, 'Train Final Accuracy': final, 'Test Initial Accuracy': test_init, 'Test Final Accuracy': test_final}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(train_indices, test_indices):\n",
    "    # visualization\n",
    "\n",
    "    # load zip code shapefile\n",
    "    zips = gpd.read_file('ZIP_CODE_040114.shp')\n",
    "    zips['ZIPCODE'] = pd.to_numeric(zips['ZIPCODE'])\n",
    "    zips = zips.to_crs(epsg=4326)\n",
    "\n",
    "    # get unique zipcodes from LEHD data and merge with above shapefile\n",
    "    zipsAll = data.groupby(by='origin', as_index=False).sum()\n",
    "    zipsAll = zipsAll.merge(zips, left_on='origin', right_on='ZIPCODE')\n",
    "    zipsAll.drop_duplicates(subset=['origin'], inplace=True)\n",
    "    zipsAll.reset_index(drop=True, inplace=True)\n",
    "    zipsAll = gpd.GeoDataFrame(zipsAll, crs=\"EPSG:4326\", geometry='geometry')\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(20,12))\n",
    "\n",
    "    # first plot\n",
    "    # borough shapefile\n",
    "    boro = gpd.read_file('Borough Boundaries.zip')\n",
    "    boro.plot(figsize=(10,10), ax=ax[0])\n",
    "\n",
    "    colors = ['r', 'g', 'b', 'orange', 'lightgreen']\n",
    "    \n",
    "    # plot boroughs\n",
    "    for ind, b in enumerate(boro.boro_code):\n",
    "        boro[boro.boro_code == b].plot(color=colors[ind], edgecolor=\"black\", ax=ax[0])\n",
    "        \n",
    "    # plot masked zips - train set\n",
    "    zipsMasked_train = zipsAll[zipsAll.index.isin(train_indices)][['origin', 'geometry']]\n",
    "\n",
    "    zipsMasked_train.plot(facecolor='lightgrey',hatch='///', edgecolor=\"white\", linestyle='--', alpha=0.8,\n",
    "                          ax=ax[0], label='masked zipcodes (train)')\n",
    "    \n",
    "    # plot masked zips - test\n",
    "    zipsMasked_test = zipsAll[zipsAll.index.isin(test_indices)][['origin', 'geometry']]\n",
    "\n",
    "    zipsMasked_test.plot(facecolor='lightgrey',hatch='///', edgecolor=\"black\", linestyle='--', alpha=0.7,\n",
    "                         ax=ax[0], label='masked zipcodes (test)')\n",
    "\n",
    "    LegendElement = [mpatches.Patch(facecolor='lightgrey', hatch='//', \n",
    "                                    edgecolor=\"black\", linestyle='--', label='masked zipcodes (test)'), \n",
    "                    mpatches.Patch(facecolor='lightgrey', hatch='//', \n",
    "                                    edgecolor=\"white\", linestyle='--', label='masked zipcodes (train)')]\n",
    "    ax[0].axis('off')\n",
    "    ax[0].legend(handles = LegendElement, loc='upper left')\n",
    "    ax[0].set_title('borough delineations with masked zipcodes')\n",
    "\n",
    "    # second plot\n",
    "    # plot boroughs\n",
    "    boro.plot(figsize=(10,10), ax=ax[1])\n",
    "\n",
    "    colors = ['r', 'g', 'b', 'orange', 'lightgreen', 'r']\n",
    "\n",
    "    for ind, b in enumerate(boro.boro_code):\n",
    "        boro[boro.boro_code == b].plot(color=colors[ind], edgecolor=\"black\", ax=ax[1])\n",
    "        \n",
    "    # get predictions for masked zips\n",
    "    zipsMasked_predTest = zipsAll[zipsAll.index.isin(test_indices)][['origin', 'geometry']]\n",
    "\n",
    "    pred = torch.argmax(op[0], dim=1).numpy()+1\n",
    "    zipsMasked_predTest['pred'] = pred[np.sort(test_indices)]\n",
    "\n",
    "    # plot masked zips\n",
    "    for p in zipsMasked_predTest.pred.unique():\n",
    "        zipsMasked_predTest[zipsMasked_predTest.pred == p].plot(color=colors[p], edgecolor=\"black\", \n",
    "                                                                linestyle='--', ax=ax[1])\n",
    "\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('predictions for masked zipcodes (test set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8b19f25209a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_random_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-9563abed0db5>\u001b[0m in \u001b[0;36mvisualize\u001b[1;34m(train_indices, test_indices)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# load zip code shapefile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mzips\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ZIP_CODE_040114.shp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mzips\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ZIPCODE'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzips\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ZIPCODE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mzips\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzips\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4326\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gpd' is not defined"
     ]
    }
   ],
   "source": [
    "visualize(random_indices, test_random_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Basics can be found here:\n",
    "\n",
    "#### Introduction to Pytorch Tensors : https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "#### Calculating gradients using Autograd : https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html\n",
    "#### Building Pytorch Models : https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html\n",
    "#### Training Pytorch Models : https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
