{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City district delineation using Graph Neural Networks\n",
    "\n",
    "### Graph Neural Networks\n",
    "\n",
    "\n",
    "Graph Neural Networks (GNNs) are a class of deep learning methods designed to perform inference on data described by graphs. GNNs are neural networks that can be directly applied to graphs, and provide an easy way to do node-level, edge-level, and graph-level prediction tasks. \n",
    "\n",
    "Anything that is composed of linked entities can be represented as a graph. Graphs are excellent tools to visualize relations between people, objects, and concepts. With graphs becoming more pervasive and richer with information, and artificial neural networks becoming more popular and capable, GNNs have become a powerful tool for many important applications.\n",
    "\n",
    "<img src='GNN.png' width=\"600\" height=\"600\">\n",
    "\n",
    "Graph neural networks can be created like any other neural network, using fully connected layers, convolutional layers, pooling layers, etc. The type and number of layers depend on the type and complexity of the graph data and the desired output.\n",
    "\n",
    "The GNN receives the formatted graph data as input and produces a vector of numerical values that represent relevant information about nodes and their relations. This vector representation is called “graph embedding.”\n",
    "\n",
    "One very popular GNN architecture is the graph convolutional neural network (GCN), which uses convolution layers to create graph embeddings. (Kipf, T.N. and Welling, M., 2016. Semi-supervised classification with graph convolutional networks. https://arxiv.org/pdf/1609.02907.pdf)\n",
    "\n",
    "In the recent years, a lot of work has been done on the problem of generalizing neural networks to work on arbitrarily structured graphs - Bruna, J., Zaremba, W., Szlam, A. and LeCun, Y., 2013. Spectral networks and locally connected networks on graphs. https://arxiv.org/pdf/1312.6203.pdf%20http://arxiv.org/abs/1312.6203.pdf,  Henaff, M., Bruna, J. and LeCun, Y., 2015. Deep convolutional networks on graph-structured data. https://arxiv.org/abs/1506.05163 etc.\n",
    "\n",
    " \n",
    "Few applications for graph neural networks:\n",
    "\n",
    "- Node classification: One of the powerful applications of GNNs is adding new information to nodes or filling gaps where information is missing. For example, say you are running a social network and you have spotted a few bot accounts. Now you want to find out if there are other bot accounts in your network. You can train a GNN to classify other users in the social network as “bot” or “not bot” based on how close their graph embeddings are to those of the known bots.\n",
    "\n",
    "- Edge prediction: Another way to put GNNs to use is to find new edges that can add value to the graph. Going back to our social network, a GNN can find users (nodes) who are close to you in embedding space but who aren’t your friends yet (i.e., there isn’t an edge connecting you to each other). These users can then be introduced to you as friend suggestions.\n",
    "\n",
    "- Clustering: GNNs can glean new structural information from graphs. For example, in a social network where everyone is in one way or another related to others (through friends, or friends of friends, etc.), the GNN can find nodes that form clusters in the embedding space.\n",
    "\n",
    "A comprehensive tutorial on GNNs by Stanford is available on Youtube: https://www.youtube.com/watch?v=JAB_plj2rbA&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn\n",
    "\n",
    "### GNN for city delineation\n",
    "\n",
    "Here, we demonstrate an application for GNN for city borough delineation. The relationships among various entities such as interaction among people, intra-city mobility, social media interactions can be interpreted in terms of graphs with many features associated with nodes, edges. \n",
    "\n",
    "We will use the LEHD mobility network among zip codes in NYC and use it to learn the corresponding borough of zip codes (nodes) in the city. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data overview\n",
    "\n",
    "The LEHD mobility matrix contains the commute numbers between the home and work zip codes of the population..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>weight</th>\n",
       "      <th>initialFeat</th>\n",
       "      <th>true_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11436</td>\n",
       "      <td>10009</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11436</td>\n",
       "      <td>10011</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11436</td>\n",
       "      <td>10013</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11436</td>\n",
       "      <td>10019</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11436</td>\n",
       "      <td>10021</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin  destination  weight  initialFeat  true_label\n",
       "0   11436        10009       1          4.0           4\n",
       "1   11436        10011       1          4.0           4\n",
       "2   11436        10013       1          4.0           4\n",
       "3   11436        10019       1          4.0           4\n",
       "4   11436        10021       1          4.0           4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/zips_merged.csv', delimiter=',')\n",
    "data = data.rename(columns={'total': 'weight', 'w_zip':'origin', 'h_zip':'destination'})\n",
    "data = data[data.destination.isin(data.origin.unique())]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the population feature into the dataset\n",
    "\n",
    "populationByAge = pd.read_csv('data/zicode_populationByAge.csv', delimiter=',')\n",
    "populationByAge = populationByAge.iloc[:,0:2]\n",
    "populationByAge.rename(columns={'ZIPCODE':'destination', 'Estimate!!Total!!Total population' : 'population' }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the house price feature into the dataset\n",
    "\n",
    "housePrice = pd.read_csv('data/zipcode_housePrice.csv', delimiter=',')\n",
    "weights = [5000,12500,17500,22500,27500,32500,37500,45000,55000,65000,75000,85000\n",
    "                    ,95000,112500,137500,162500,187500,225000,275000,350000,450000,625000,875000\n",
    "                        ,1250000,1750000,2000000]\n",
    "for i in range(len(weights)):\n",
    "    housePrice.iloc[i,2:] = housePrice.iloc[i,2:]*weights[i]\n",
    "housePrice.iloc[:,1] [housePrice.iloc[:,1] == 0] = 1\n",
    "\n",
    "tmp = (housePrice.iloc[:,2:] != 0 ).sum(axis=1)\n",
    "tmp[tmp == 0] = 1\n",
    "housePrice = pd.concat([housePrice.iloc[:,0],housePrice.iloc[:,2:].sum(axis=1) / tmp ], axis = 1 )\n",
    "housePrice.rename(columns={'ZIPCODE':'destination', 0: 'house_price'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the area size feature into the dataset\n",
    "\n",
    "area = pd.read_csv('data/zips_area.csv', delimiter=',')\n",
    "area = area.iloc[:,:2]\n",
    "area.rename(columns={'ZIPCODE':'destination', 'AREA' : 'area'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the income level feature into the dataset\n",
    "\n",
    "income = pd.read_csv('data/zipcode_income.csv', delimiter=',')\n",
    "income = income.iloc[:,:2]\n",
    "income[income.isna()] = 0\n",
    "income.rename(columns={'ZIPCODE':'destination', 'median_familyIncome(USD)' : 'income'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the population jobs feature into the dataset\n",
    "\n",
    "populationJobs = pd.read_csv('data/zipcode_population_Jobs.csv', delimiter=',')\n",
    "populationJobs = populationJobs.iloc[:,:2]\n",
    "populationJobs.rename(columns={'ZIPCODE':'destination', 'totalJobs' : 'jobs'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(populationByAge)\n",
    "data = data.merge(housePrice)\n",
    "data = data.merge(area)\n",
    "data = data.merge(income)\n",
    "data = data.merge(populationJobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "\n",
    "data.iloc[:,5:]=(data.iloc[:,5:]-data.iloc[:,5:].min())/(data.iloc[:,5:].max()-data.iloc[:,5:].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>weight</th>\n",
       "      <th>initialFeat</th>\n",
       "      <th>true_label</th>\n",
       "      <th>population</th>\n",
       "      <th>house_price</th>\n",
       "      <th>area</th>\n",
       "      <th>income</th>\n",
       "      <th>jobs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11436</td>\n",
       "      <td>10009</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.300165</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.032183</td>\n",
       "      <td>0.243977</td>\n",
       "      <td>0.042630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11213</td>\n",
       "      <td>10009</td>\n",
       "      <td>14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.300165</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.032183</td>\n",
       "      <td>0.243977</td>\n",
       "      <td>0.042630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11212</td>\n",
       "      <td>10009</td>\n",
       "      <td>27</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.300165</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.032183</td>\n",
       "      <td>0.243977</td>\n",
       "      <td>0.042630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11225</td>\n",
       "      <td>10009</td>\n",
       "      <td>26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.300165</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.032183</td>\n",
       "      <td>0.243977</td>\n",
       "      <td>0.042630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11218</td>\n",
       "      <td>10009</td>\n",
       "      <td>60</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.300165</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.032183</td>\n",
       "      <td>0.243977</td>\n",
       "      <td>0.042630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36407</th>\n",
       "      <td>11211</td>\n",
       "      <td>11371</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36408</th>\n",
       "      <td>11373</td>\n",
       "      <td>11371</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36409</th>\n",
       "      <td>10168</td>\n",
       "      <td>11371</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36410</th>\n",
       "      <td>10278</td>\n",
       "      <td>11371</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36411</th>\n",
       "      <td>10036</td>\n",
       "      <td>11371</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36412 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       origin  destination  weight  initialFeat  true_label  population  \\\n",
       "0       11436        10009       1          4.0           4    0.300165   \n",
       "1       11213        10009      14          3.0           3    0.300165   \n",
       "2       11212        10009      27          3.0           3    0.300165   \n",
       "3       11225        10009      26          3.0           3    0.300165   \n",
       "4       11218        10009      60          3.0           3    0.300165   \n",
       "...       ...          ...     ...          ...         ...         ...   \n",
       "36407   11211        11371       7          1.0           3    0.000000   \n",
       "36408   11373        11371       3          4.0           4    0.000000   \n",
       "36409   10168        11371       1          1.0           1    0.000000   \n",
       "36410   10278        11371       1          1.0           1    0.000000   \n",
       "36411   10036        11371       5          1.0           1    0.000000   \n",
       "\n",
       "       house_price      area    income      jobs  \n",
       "0         0.005332  0.032183  0.243977  0.042630  \n",
       "1         0.005332  0.032183  0.243977  0.042630  \n",
       "2         0.005332  0.032183  0.243977  0.042630  \n",
       "3         0.005332  0.032183  0.243977  0.042630  \n",
       "4         0.005332  0.032183  0.243977  0.042630  \n",
       "...            ...       ...       ...       ...  \n",
       "36407     0.000000  0.063146  0.000000  0.050874  \n",
       "36408     0.000000  0.063146  0.000000  0.050874  \n",
       "36409     0.000000  0.063146  0.000000  0.050874  \n",
       "36410     0.000000  0.063146  0.000000  0.050874  \n",
       "36411     0.000000  0.063146  0.000000  0.050874  \n",
       "\n",
       "[36412 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial model config\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "weight_decay = 10e-4\n",
    "epochs = 10001\n",
    "seed = 165\n",
    "hidden = 10\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a symmetric normalization for the propogating the layer, i.e. $$D^{−1/2}AD^{−1/2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(adj):\n",
    "\n",
    "    adj = torch.FloatTensor(adj)\n",
    "    adj_id = torch.FloatTensor(torch.eye(adj.shape[1]))\n",
    "    adj_id = adj_id.reshape((1, adj.shape[1], adj.shape[1]))\n",
    "    adj_id = adj_id.repeat(adj.shape[0], 1, 1)\n",
    "    adj = adj + adj_id\n",
    "    rowsum = torch.FloatTensor(adj.sum(2))\n",
    "    degree_mat_inv_sqrt = torch.diag_embed(torch.float_power(rowsum,-0.5), dim1=-2, dim2=-1).float()\n",
    "    adj_norm = torch.bmm(torch.transpose(torch.bmm(adj,degree_mat_inv_sqrt),1,2),degree_mat_inv_sqrt)\n",
    "\n",
    "    return adj_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function for the GNN is the double ReLU which is linear between between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doublerelu(x):\n",
    "    return torch.clamp(x, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Layer propogation rule of the GNN is : $$ H^{(l+1)} = f(H^{(l)}, A) = \\sigma ( H^{(l)}W_1^{(l)} + \\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W_2^{(l)}) $$\n",
    "\n",
    "where H is the l'th neural network layer, A is the adjaceny matrix, D is the diagonal node degree matrix, W1 , W2 are learnable weight matrices initialised as W1 = 1, W2 = 0  and sigma is the activation function doublerelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN1Layer(Module):\n",
    "\n",
    "    def __init__(self, batch_size, in_features, out_features, first):\n",
    "        super(GNN1Layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialse W1 = 1, W2 = 0 as pytorch learnable weights (parameters) that have require_grad = True which is\n",
    "        # required for calculating gradients while backpropogating using gradient descent\n",
    "        weight1_eye = torch.FloatTensor(torch.eye(in_features, out_features))\n",
    "        weight1_eye = weight1_eye.reshape((1, in_features, out_features))\n",
    "        weight1_eye = weight1_eye.repeat(batch_size, 1, 1)\n",
    "        self.weight1 = Parameter(weight1_eye)\n",
    "        if not first:\n",
    "            self.weight2 = Parameter(torch.zeros(batch_size, in_features, out_features))\n",
    "        else:\n",
    "            self.weight2 = Parameter(torch.empty(batch_size, in_features, out_features))\n",
    "            nn.init.kaiming_normal_(self.weight2, mode='fan_out')\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # first term H*W1\n",
    "        v1 = torch.bmm(input, self.weight1)\n",
    "        # second term adj_norm*H*W2\n",
    "        v2 = torch.bmm(torch.bmm(adj, input), self.weight2)\n",
    "        # adding the two terms\n",
    "        output = v1 + v2\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN1(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, nfeat, ndim, hidden, first):\n",
    "        super(GNN1, self).__init__()\n",
    "\n",
    "        self.gc1 = GNN1Layer(batch_size, nfeat, ndim, first)\n",
    "\n",
    "    def forward(self, x, adj, random_indices):\n",
    "        f = torch.clone(x)\n",
    "        # Applying activation function sigma (doublerelu) on the layer propogation\n",
    "        x = doublerelu(self.gc1(x, adj))\n",
    "        x = x/x.sum(axis=2).unsqueeze(2) #normalize st sum = 1\n",
    "        \n",
    "        # Only the train nodes are updated in backpropogation \n",
    "        f[0][random_indices, :x.shape[2]] = x[0][random_indices, :]\n",
    "        \n",
    "        return f[:,:,:x.shape[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(adj,features,labels,random_indices,val_indices,val_features,first=False):\n",
    "    \n",
    "    # calculate symmetric normalisation for layer propogation\n",
    "    adj_norm = normalize(adj)\n",
    "    \n",
    "    labels = labels - 1\n",
    "    \n",
    "    # Convert from numpy to torch tensors\n",
    "    adj = torch.FloatTensor(adj)\n",
    "    adj_norm = torch.FloatTensor(adj_norm)\n",
    "    features = torch.FloatTensor(features)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "    val_features = torch.FloatTensor(val_features)\n",
    "    \n",
    "    # initialise the mode\n",
    "    model = GNN1(batch_size=adj.shape[0],\n",
    "                nfeat=features.shape[-1],\n",
    "                ndim=nb_label,\n",
    "                hidden=hidden,\n",
    "                first=first)\n",
    "    \n",
    "    # Transfer the weights to GPU for training\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "        features = features.cuda()\n",
    "        adj = adj.cuda()\n",
    "        adj_norm = adj_norm.cuda()\n",
    "        labels = labels.cuda()\n",
    "        val_features = val_features.cuda()\n",
    "    \n",
    "    # Train model\n",
    "    t_total = time.time()\n",
    "\n",
    "    # Using adam optimizers for backpropogation\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # loss function criteria is cross entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train for the no of epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        # Pytorch accumulates gradient after every operation on tensors (defined by the model architecture)\n",
    "        # with require_grad = True. With each new epoch, we need to reset this gradient to 0 to calculate gradient\n",
    "        # for this epoch.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get the output from forward propogation of our model\n",
    "        output = model(features, adj_norm, random_indices)\n",
    "\n",
    "        # Calculate Train accuracy\n",
    "        train_output = output[:,random_indices,:]\n",
    "        train_labels = labels[random_indices,:]\n",
    "        train_accuracy = torch.sum(torch.argmax(train_output,axis=2)==train_labels.reshape(1,-1))/train_labels.shape[0]\n",
    "        \n",
    "        # Calculate Validation accuracy\n",
    "        with torch.no_grad():\n",
    "            val_output = model(val_features, adj_norm, val_indices)\n",
    "            val_labels = labels[val_indices,:]\n",
    "            val_accuracy = torch.sum(torch.argmax(val_output[0][val_indices, :],axis=1)==labels[val_indices, :].reshape(1,-1))/labels[val_indices, :].shape[0]\n",
    "        \n",
    "        # Calculate the loss between our models training output and true label\n",
    "        loss = criterion(output[0],labels.reshape(-1).long())\n",
    "        \n",
    "        # Calculate the gradients \n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print summary of training \n",
    "        if epoch == 0:\n",
    "            best_loss = loss\n",
    "            best_output = output\n",
    "            best_acc = train_accuracy\n",
    "            best_val_acc = val_accuracy\n",
    "            best_val_output = val_output\n",
    "        else:\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_output = output\n",
    "                best_acc = train_accuracy\n",
    "                best_val_acc = val_accuracy\n",
    "                best_val_output = val_output\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch: {:04d}'.format(epoch + 1),\n",
    "                  'Train Accuracy: {:.4f}'.format(best_acc.item()),\n",
    "                  'Validation Accuracy: {:.4f}'.format(best_val_acc.item()),\n",
    "                  'Loss: {:.8f}'.format(best_loss.item()),\n",
    "                  'time: {:.4f}s'.format(time.time() - t))\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    \n",
    "    return best_loss,best_output,best_val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svdApprox(adj, dim, relu=False):\n",
    "    adj = torch.FloatTensor(adj[0])\n",
    "    U, S, Vh = torch.linalg.svd(adj)\n",
    "    mu = torch.matmul(torch.matmul(U[:, :dim], torch.diag(S[:dim])), Vh[:dim, :])\n",
    "\n",
    "    embedx = torch.matmul(U[:, :dim], torch.diag(torch.pow(S[:dim], 0.5)))\n",
    "    embedy = torch.transpose(torch.matmul(torch.diag(torch.pow(S[:dim], 0.5)), Vh[:dim, :]), 0, 1)\n",
    "\n",
    "    return embedx, embedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading data, returns adjacency matrix, initial feature assignments and true labels\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    G = nx.from_pandas_edgelist(data, 'origin', 'destination', 'weight',create_using=nx.DiGraph())\n",
    "    adj_list = np.array([nx.adjacency_matrix(G).todense()], dtype=float)\n",
    "    \n",
    "    init_feat1 = np.array(data.groupby('origin')['population'].agg(['unique']))\n",
    "    init_feat1 = np.array(list(map(lambda x: x[0][0], init_feat1))).reshape(-1, 1)\n",
    "    init_feat2 = np.array(data.groupby('origin')['house_price'].agg(['unique']))\n",
    "    init_feat2 = np.array(list(map(lambda x: x[0][0], init_feat2))).reshape(-1, 1)\n",
    "    init_feat3 = np.array(data.groupby('origin')['area'].agg(['unique']))\n",
    "    init_feat3 = np.array(list(map(lambda x: x[0][0], init_feat3))).reshape(-1, 1)\n",
    "    init_feat4 = np.array(data.groupby('origin')['income'].agg(['unique']))\n",
    "    init_feat4 = np.array(list(map(lambda x: x[0][0], init_feat4))).reshape(-1, 1)\n",
    "    init_feat5 = np.array(data.groupby('origin')['jobs'].agg(['unique']))\n",
    "    init_feat5 = np.array(list(map(lambda x: x[0][0], init_feat5))).reshape(-1, 1)\n",
    "    \n",
    "    init_feat = np.concatenate([init_feat1,init_feat2,init_feat3,init_feat4,init_feat5],axis=1)\n",
    "    \n",
    "    true_label = np.array(data.groupby('origin')['true_label'].agg(['unique']))\n",
    "    \n",
    "    true_label = np.array(list(map(lambda x: x[0][0], true_label))).reshape(-1, 1)\n",
    "    return adj_list,init_feat,true_label\n",
    "\n",
    "adj,feature,labels = load_data()\n",
    "\n",
    "features = np.expand_dims(feature, axis=0)\n",
    "val_features = np.copy(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_feat = np.array(data.groupby('origin')['initialFeat'].agg(['unique']))\n",
    "init_feat = np.array(list(map(lambda x: x[0][0], init_feat))).reshape(-1, 1)\n",
    "init_feat = init_feat - 1\n",
    "nb_label = int(max(init_feat)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Train Accuracy: 0.2711 Validation Accuracy: 0.1429 Loss: 1.58509779 time: 0.4287s\n",
      "Epoch: 1001 Train Accuracy: 0.3735 Validation Accuracy: 0.3333 Loss: 1.56856894 time: 0.0030s\n",
      "Epoch: 2001 Train Accuracy: 0.3916 Validation Accuracy: 0.3571 Loss: 1.55656838 time: 0.0040s\n",
      "Epoch: 3001 Train Accuracy: 0.3313 Validation Accuracy: 0.2381 Loss: 1.50360656 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.3735 Validation Accuracy: 0.2143 Loss: 1.48460150 time: 0.0030s\n",
      "Epoch: 5001 Train Accuracy: 0.5060 Validation Accuracy: 0.4286 Loss: 1.47094071 time: 0.0040s\n",
      "Epoch: 6001 Train Accuracy: 0.5060 Validation Accuracy: 0.4286 Loss: 1.45996392 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.5120 Validation Accuracy: 0.4286 Loss: 1.44852006 time: 0.0040s\n",
      "Epoch: 8001 Train Accuracy: 0.5120 Validation Accuracy: 0.4286 Loss: 1.42689407 time: 0.0040s\n",
      "Epoch: 9001 Train Accuracy: 0.5120 Validation Accuracy: 0.4286 Loss: 1.42689407 time: 0.0030s\n",
      "Epoch: 10001 Train Accuracy: 0.5120 Validation Accuracy: 0.4286 Loss: 1.42689407 time: 0.0040s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 33.9847s\n",
      "Epoch: 0001 Train Accuracy: 0.5120 Validation Accuracy: 0.4286 Loss: 1.42689407 time: 0.0040s\n",
      "Epoch: 1001 Train Accuracy: 0.5181 Validation Accuracy: 0.4286 Loss: 1.42156053 time: 0.0030s\n",
      "Epoch: 2001 Train Accuracy: 0.5120 Validation Accuracy: 0.4286 Loss: 1.41657805 time: 0.0040s\n",
      "Epoch: 3001 Train Accuracy: 0.5241 Validation Accuracy: 0.4286 Loss: 1.40432608 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.5241 Validation Accuracy: 0.4286 Loss: 1.40432608 time: 0.0030s\n",
      "Epoch: 5001 Train Accuracy: 0.5241 Validation Accuracy: 0.4286 Loss: 1.40432608 time: 0.0030s\n",
      "Epoch: 6001 Train Accuracy: 0.5241 Validation Accuracy: 0.4286 Loss: 1.40432608 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.5241 Validation Accuracy: 0.4286 Loss: 1.40432608 time: 0.0040s\n",
      "Epoch: 8001 Train Accuracy: 0.5241 Validation Accuracy: 0.4286 Loss: 1.40432608 time: 0.0040s\n",
      "Epoch: 9001 Train Accuracy: 0.5241 Validation Accuracy: 0.4286 Loss: 1.40432608 time: 0.0030s\n",
      "Epoch: 10001 Train Accuracy: 0.5241 Validation Accuracy: 0.4286 Loss: 1.40432608 time: 0.0038s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 35.1013s\n",
      "Epoch: 0001 Train Accuracy: 0.5241 Validation Accuracy: 0.4286 Loss: 1.40432608 time: 0.0040s\n",
      "Epoch: 1001 Train Accuracy: 0.5301 Validation Accuracy: 0.4524 Loss: 1.39641786 time: 0.0030s\n",
      "Epoch: 2001 Train Accuracy: 0.5482 Validation Accuracy: 0.4524 Loss: 1.38846219 time: 0.0040s\n",
      "Epoch: 3001 Train Accuracy: 0.5482 Validation Accuracy: 0.4286 Loss: 1.38253224 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.5602 Validation Accuracy: 0.4286 Loss: 1.37594163 time: 0.0040s\n",
      "Epoch: 5001 Train Accuracy: 0.5663 Validation Accuracy: 0.4286 Loss: 1.36638820 time: 0.0040s\n",
      "Epoch: 6001 Train Accuracy: 0.5843 Validation Accuracy: 0.4286 Loss: 1.36159277 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.5843 Validation Accuracy: 0.4286 Loss: 1.36159277 time: 0.0030s\n",
      "Epoch: 8001 Train Accuracy: 0.5843 Validation Accuracy: 0.4286 Loss: 1.36159277 time: 0.0040s\n",
      "Epoch: 9001 Train Accuracy: 0.5843 Validation Accuracy: 0.4286 Loss: 1.36159277 time: 0.0040s\n",
      "Epoch: 10001 Train Accuracy: 0.5843 Validation Accuracy: 0.4286 Loss: 1.36159277 time: 0.0040s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 35.4697s\n",
      "Epoch: 0001 Train Accuracy: 0.5843 Validation Accuracy: 0.4286 Loss: 1.36159277 time: 0.0040s\n",
      "Epoch: 1001 Train Accuracy: 0.5904 Validation Accuracy: 0.4286 Loss: 1.35403609 time: 0.0050s\n",
      "Epoch: 2001 Train Accuracy: 0.6024 Validation Accuracy: 0.4286 Loss: 1.34794211 time: 0.0030s\n",
      "Epoch: 3001 Train Accuracy: 0.6145 Validation Accuracy: 0.4286 Loss: 1.34188199 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.6205 Validation Accuracy: 0.4286 Loss: 1.33957934 time: 0.0030s\n",
      "Epoch: 5001 Train Accuracy: 0.6325 Validation Accuracy: 0.4286 Loss: 1.33621836 time: 0.0040s\n",
      "Epoch: 6001 Train Accuracy: 0.6265 Validation Accuracy: 0.4286 Loss: 1.33237183 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.6265 Validation Accuracy: 0.4524 Loss: 1.33042252 time: 0.0040s\n",
      "Epoch: 8001 Train Accuracy: 0.6205 Validation Accuracy: 0.4762 Loss: 1.32741237 time: 0.0040s\n",
      "Epoch: 9001 Train Accuracy: 0.6325 Validation Accuracy: 0.4762 Loss: 1.32307482 time: 0.0040s\n",
      "Epoch: 10001 Train Accuracy: 0.6325 Validation Accuracy: 0.4762 Loss: 1.32305086 time: 0.0050s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 37.8799s\n",
      "Epoch: 0001 Train Accuracy: 0.6325 Validation Accuracy: 0.4762 Loss: 1.32305086 time: 0.0040s\n",
      "Epoch: 1001 Train Accuracy: 0.6325 Validation Accuracy: 0.4524 Loss: 1.32145739 time: 0.0040s\n",
      "Epoch: 2001 Train Accuracy: 0.6386 Validation Accuracy: 0.4524 Loss: 1.31973755 time: 0.0030s\n",
      "Epoch: 3001 Train Accuracy: 0.6386 Validation Accuracy: 0.4286 Loss: 1.31793928 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.6446 Validation Accuracy: 0.4286 Loss: 1.31589890 time: 0.0050s\n",
      "Epoch: 5001 Train Accuracy: 0.6446 Validation Accuracy: 0.4286 Loss: 1.31379485 time: 0.0040s\n",
      "Epoch: 6001 Train Accuracy: 0.6506 Validation Accuracy: 0.4286 Loss: 1.31126237 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.6506 Validation Accuracy: 0.4286 Loss: 1.30535245 time: 0.0040s\n",
      "Epoch: 8001 Train Accuracy: 0.6687 Validation Accuracy: 0.4286 Loss: 1.29950953 time: 0.0030s\n",
      "Epoch: 9001 Train Accuracy: 0.6867 Validation Accuracy: 0.4524 Loss: 1.29495704 time: 0.0030s\n",
      "Epoch: 10001 Train Accuracy: 0.6867 Validation Accuracy: 0.4524 Loss: 1.29495704 time: 0.0030s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 36.8051s\n",
      "Epoch: 0001 Train Accuracy: 0.6867 Validation Accuracy: 0.4524 Loss: 1.29495704 time: 0.0050s\n",
      "Epoch: 1001 Train Accuracy: 0.6867 Validation Accuracy: 0.4524 Loss: 1.29281151 time: 0.0040s\n",
      "Epoch: 2001 Train Accuracy: 0.6867 Validation Accuracy: 0.4524 Loss: 1.29086089 time: 0.0040s\n",
      "Epoch: 3001 Train Accuracy: 0.6867 Validation Accuracy: 0.4524 Loss: 1.28933609 time: 0.0030s\n",
      "Epoch: 4001 Train Accuracy: 0.6807 Validation Accuracy: 0.4762 Loss: 1.28784704 time: 0.0040s\n",
      "Epoch: 5001 Train Accuracy: 0.6867 Validation Accuracy: 0.4762 Loss: 1.28613007 time: 0.0030s\n",
      "Epoch: 6001 Train Accuracy: 0.6928 Validation Accuracy: 0.4762 Loss: 1.28387201 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.6928 Validation Accuracy: 0.4762 Loss: 1.28358674 time: 0.0040s\n",
      "Epoch: 8001 Train Accuracy: 0.6928 Validation Accuracy: 0.4762 Loss: 1.28358674 time: 0.0040s\n",
      "Epoch: 9001 Train Accuracy: 0.6928 Validation Accuracy: 0.4762 Loss: 1.28358674 time: 0.0040s\n",
      "Epoch: 10001 Train Accuracy: 0.6928 Validation Accuracy: 0.4762 Loss: 1.28358674 time: 0.0040s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 34.9865s\n",
      "Epoch: 0001 Train Accuracy: 0.6928 Validation Accuracy: 0.4762 Loss: 1.28358674 time: 0.0050s\n",
      "Epoch: 1001 Train Accuracy: 0.6928 Validation Accuracy: 0.4762 Loss: 1.28262639 time: 0.0030s\n",
      "Epoch: 2001 Train Accuracy: 0.6867 Validation Accuracy: 0.4762 Loss: 1.28157938 time: 0.0030s\n",
      "Epoch: 3001 Train Accuracy: 0.6867 Validation Accuracy: 0.4762 Loss: 1.28027475 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.6867 Validation Accuracy: 0.4762 Loss: 1.27876031 time: 0.0030s\n",
      "Epoch: 5001 Train Accuracy: 0.6867 Validation Accuracy: 0.4762 Loss: 1.27713907 time: 0.0040s\n",
      "Epoch: 6001 Train Accuracy: 0.6988 Validation Accuracy: 0.4762 Loss: 1.27496910 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.6988 Validation Accuracy: 0.4762 Loss: 1.27462947 time: 0.0030s\n",
      "Epoch: 8001 Train Accuracy: 0.6988 Validation Accuracy: 0.4762 Loss: 1.27462947 time: 0.0040s\n",
      "Epoch: 9001 Train Accuracy: 0.6988 Validation Accuracy: 0.4762 Loss: 1.27462947 time: 0.0040s\n",
      "Epoch: 10001 Train Accuracy: 0.6988 Validation Accuracy: 0.4762 Loss: 1.27462947 time: 0.0040s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 34.6259s\n",
      "Epoch: 0001 Train Accuracy: 0.6988 Validation Accuracy: 0.4762 Loss: 1.27462947 time: 0.0040s\n",
      "Epoch: 1001 Train Accuracy: 0.6988 Validation Accuracy: 0.4762 Loss: 1.27349806 time: 0.0040s\n",
      "Epoch: 2001 Train Accuracy: 0.6988 Validation Accuracy: 0.4762 Loss: 1.27240515 time: 0.0030s\n",
      "Epoch: 3001 Train Accuracy: 0.7108 Validation Accuracy: 0.4762 Loss: 1.27133930 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.7108 Validation Accuracy: 0.4762 Loss: 1.27013981 time: 0.0030s\n",
      "Epoch: 5001 Train Accuracy: 0.6988 Validation Accuracy: 0.4762 Loss: 1.26854014 time: 0.0040s\n",
      "Epoch: 6001 Train Accuracy: 0.7169 Validation Accuracy: 0.4762 Loss: 1.26410604 time: 0.0040s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7001 Train Accuracy: 0.7169 Validation Accuracy: 0.4762 Loss: 1.26410604 time: 0.0030s\n",
      "Epoch: 8001 Train Accuracy: 0.7169 Validation Accuracy: 0.4762 Loss: 1.26410604 time: 0.0030s\n",
      "Epoch: 9001 Train Accuracy: 0.7169 Validation Accuracy: 0.4762 Loss: 1.26410604 time: 0.0040s\n",
      "Epoch: 10001 Train Accuracy: 0.7169 Validation Accuracy: 0.4762 Loss: 1.26410604 time: 0.0030s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 34.4270s\n",
      "Epoch: 0001 Train Accuracy: 0.7169 Validation Accuracy: 0.4762 Loss: 1.26410604 time: 0.0040s\n",
      "Epoch: 1001 Train Accuracy: 0.7108 Validation Accuracy: 0.4762 Loss: 1.26318038 time: 0.0030s\n",
      "Epoch: 2001 Train Accuracy: 0.7169 Validation Accuracy: 0.4762 Loss: 1.26209664 time: 0.0040s\n",
      "Epoch: 3001 Train Accuracy: 0.7169 Validation Accuracy: 0.4762 Loss: 1.26112413 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.7169 Validation Accuracy: 0.4762 Loss: 1.26029599 time: 0.0040s\n",
      "Epoch: 5001 Train Accuracy: 0.7169 Validation Accuracy: 0.4762 Loss: 1.25929976 time: 0.0040s\n",
      "Epoch: 6001 Train Accuracy: 0.7169 Validation Accuracy: 0.4762 Loss: 1.25742006 time: 0.0030s\n",
      "Epoch: 7001 Train Accuracy: 0.7349 Validation Accuracy: 0.4762 Loss: 1.25506139 time: 0.0030s\n",
      "Epoch: 8001 Train Accuracy: 0.7349 Validation Accuracy: 0.4762 Loss: 1.24635398 time: 0.0040s\n",
      "Epoch: 9001 Train Accuracy: 0.7349 Validation Accuracy: 0.4762 Loss: 1.24635398 time: 0.0040s\n",
      "Epoch: 10001 Train Accuracy: 0.7349 Validation Accuracy: 0.4762 Loss: 1.24635398 time: 0.0030s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 35.0711s\n",
      "Epoch: 0001 Train Accuracy: 0.7349 Validation Accuracy: 0.2143 Loss: 1.24635398 time: 0.0050s\n",
      "Epoch: 1001 Train Accuracy: 0.7349 Validation Accuracy: 0.2143 Loss: 1.24551320 time: 0.0040s\n",
      "Epoch: 2001 Train Accuracy: 0.7349 Validation Accuracy: 0.2143 Loss: 1.24478745 time: 0.0040s\n",
      "Epoch: 3001 Train Accuracy: 0.7410 Validation Accuracy: 0.2143 Loss: 1.24238908 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.7410 Validation Accuracy: 0.2143 Loss: 1.24112022 time: 0.0030s\n",
      "Epoch: 5001 Train Accuracy: 0.7410 Validation Accuracy: 0.2143 Loss: 1.24058282 time: 0.0030s\n",
      "Epoch: 6001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.24030089 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.24011779 time: 0.0030s\n",
      "Epoch: 8001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.23980510 time: 0.0042s\n",
      "Epoch: 9001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.23945928 time: 0.0040s\n",
      "Epoch: 10001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.23902059 time: 0.0040s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 35.0254s\n",
      "Epoch: 0001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.23902059 time: 0.0050s\n",
      "Epoch: 1001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.23899388 time: 0.0040s\n",
      "Epoch: 2001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.23892784 time: 0.0030s\n",
      "Epoch: 3001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.23882425 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.23867011 time: 0.0040s\n",
      "Epoch: 5001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.23841941 time: 0.0030s\n",
      "Epoch: 6001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.23804080 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.7470 Validation Accuracy: 0.2143 Loss: 1.23702753 time: 0.0040s\n",
      "Epoch: 8001 Train Accuracy: 0.7530 Validation Accuracy: 0.2143 Loss: 1.23243833 time: 0.0040s\n",
      "Epoch: 9001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.23046768 time: 0.0040s\n",
      "Epoch: 10001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.23046768 time: 0.0040s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 34.0942s\n",
      "Epoch: 0001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.23046768 time: 0.0040s\n",
      "Epoch: 1001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.23043847 time: 0.0040s\n",
      "Epoch: 2001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.23038185 time: 0.0030s\n",
      "Epoch: 3001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.23030305 time: 0.0030s\n",
      "Epoch: 4001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.23016572 time: 0.0040s\n",
      "Epoch: 5001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22977412 time: 0.0040s\n",
      "Epoch: 6001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22975445 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22975445 time: 0.0040s\n",
      "Epoch: 8001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22975445 time: 0.0040s\n",
      "Epoch: 9001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22975445 time: 0.0030s\n",
      "Epoch: 10001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22975445 time: 0.0034s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 34.2174s\n",
      "Epoch: 0001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22975445 time: 0.0030s\n",
      "Epoch: 1001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22969484 time: 0.0040s\n",
      "Epoch: 2001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22958469 time: 0.0040s\n",
      "Epoch: 3001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22941065 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22914171 time: 0.0040s\n",
      "Epoch: 5001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22877705 time: 0.0030s\n",
      "Epoch: 6001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22873509 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22873509 time: 0.0030s\n",
      "Epoch: 8001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22873509 time: 0.0040s\n",
      "Epoch: 9001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22873509 time: 0.0030s\n",
      "Epoch: 10001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22873509 time: 0.0030s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 34.3238s\n",
      "Epoch: 0001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22873509 time: 0.0040s\n",
      "Epoch: 1001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22869468 time: 0.0030s\n",
      "Epoch: 2001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22859740 time: 0.0030s\n",
      "Epoch: 3001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22847438 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.7590 Validation Accuracy: 0.2143 Loss: 1.22834587 time: 0.0040s\n",
      "Epoch: 5001 Train Accuracy: 0.7530 Validation Accuracy: 0.2143 Loss: 1.22817051 time: 0.0040s\n",
      "Epoch: 6001 Train Accuracy: 0.7530 Validation Accuracy: 0.2143 Loss: 1.22772670 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.7530 Validation Accuracy: 0.2143 Loss: 1.22715211 time: 0.0040s\n",
      "Epoch: 8001 Train Accuracy: 0.7530 Validation Accuracy: 0.2143 Loss: 1.22686696 time: 0.0040s\n",
      "Epoch: 9001 Train Accuracy: 0.7530 Validation Accuracy: 0.2143 Loss: 1.22686696 time: 0.0040s\n",
      "Epoch: 10001 Train Accuracy: 0.7530 Validation Accuracy: 0.2143 Loss: 1.22686696 time: 0.0040s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 35.0029s\n",
      "Epoch: 0001 Train Accuracy: 0.7530 Validation Accuracy: 0.2143 Loss: 1.22686696 time: 0.0040s\n",
      "Epoch: 1001 Train Accuracy: 0.7530 Validation Accuracy: 0.2143 Loss: 1.22645998 time: 0.0030s\n",
      "Epoch: 2001 Train Accuracy: 0.7530 Validation Accuracy: 0.2143 Loss: 1.22626626 time: 0.0030s\n",
      "Epoch: 3001 Train Accuracy: 0.7530 Validation Accuracy: 0.2143 Loss: 1.22597897 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.7651 Validation Accuracy: 0.2143 Loss: 1.21894085 time: 0.0040s\n",
      "Epoch: 5001 Train Accuracy: 0.7651 Validation Accuracy: 0.2143 Loss: 1.21894085 time: 0.0040s\n",
      "Epoch: 6001 Train Accuracy: 0.7651 Validation Accuracy: 0.2143 Loss: 1.21894085 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.7651 Validation Accuracy: 0.2143 Loss: 1.21894085 time: 0.0040s\n",
      "Epoch: 8001 Train Accuracy: 0.7651 Validation Accuracy: 0.2143 Loss: 1.21894085 time: 0.0030s\n",
      "Epoch: 9001 Train Accuracy: 0.7651 Validation Accuracy: 0.2143 Loss: 1.21894085 time: 0.0040s\n",
      "Epoch: 10001 Train Accuracy: 0.7651 Validation Accuracy: 0.2143 Loss: 1.21894085 time: 0.0040s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 35.2886s\n",
      "Epoch: 0001 Train Accuracy: 0.7651 Validation Accuracy: 0.2143 Loss: 1.21894085 time: 0.0040s\n",
      "Epoch: 1001 Train Accuracy: 0.7651 Validation Accuracy: 0.2143 Loss: 1.21833730 time: 0.0034s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2001 Train Accuracy: 0.7711 Validation Accuracy: 0.2143 Loss: 1.21678996 time: 0.0040s\n",
      "Epoch: 3001 Train Accuracy: 0.7711 Validation Accuracy: 0.2143 Loss: 1.21670175 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.7711 Validation Accuracy: 0.2143 Loss: 1.21660876 time: 0.0040s\n",
      "Epoch: 5001 Train Accuracy: 0.7711 Validation Accuracy: 0.2143 Loss: 1.21651137 time: 0.0050s\n",
      "Epoch: 6001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21639669 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21625471 time: 0.0040s\n",
      "Epoch: 8001 Train Accuracy: 0.7711 Validation Accuracy: 0.2143 Loss: 1.21588218 time: 0.0030s\n",
      "Epoch: 9001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21390152 time: 0.0040s\n",
      "Epoch: 10001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21390152 time: 0.0040s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 35.5809s\n",
      "Epoch: 0001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21390152 time: 0.0040s\n",
      "Epoch: 1001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21385944 time: 0.0040s\n",
      "Epoch: 2001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21381176 time: 0.0040s\n",
      "Epoch: 3001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21373808 time: 0.0040s\n",
      "Epoch: 4001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21363056 time: 0.0050s\n",
      "Epoch: 5001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21353316 time: 0.0050s\n",
      "Epoch: 6001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21342325 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21327865 time: 0.0050s\n",
      "Epoch: 8001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21301532 time: 0.0050s\n",
      "Epoch: 9001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21232986 time: 0.0040s\n",
      "Epoch: 10001 Train Accuracy: 0.7831 Validation Accuracy: 0.2143 Loss: 1.21145642 time: 0.0040s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 41.6234s\n",
      "Epoch: 0001 Train Accuracy: 0.7831 Validation Accuracy: 0.2143 Loss: 1.21145642 time: 0.0040s\n",
      "Epoch: 1001 Train Accuracy: 0.7831 Validation Accuracy: 0.2143 Loss: 1.21144569 time: 0.0040s\n",
      "Epoch: 2001 Train Accuracy: 0.7831 Validation Accuracy: 0.2143 Loss: 1.21143198 time: 0.0040s\n",
      "Epoch: 3001 Train Accuracy: 0.7831 Validation Accuracy: 0.2143 Loss: 1.21140552 time: 0.0041s\n",
      "Epoch: 4001 Train Accuracy: 0.7831 Validation Accuracy: 0.2143 Loss: 1.21136785 time: 0.0040s\n",
      "Epoch: 5001 Train Accuracy: 0.7831 Validation Accuracy: 0.2143 Loss: 1.21131492 time: 0.0040s\n",
      "Epoch: 6001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21124065 time: 0.0040s\n",
      "Epoch: 7001 Train Accuracy: 0.7831 Validation Accuracy: 0.2143 Loss: 1.21112680 time: 0.0040s\n",
      "Epoch: 8001 Train Accuracy: 0.7771 Validation Accuracy: 0.2143 Loss: 1.21089315 time: 0.0040s\n",
      "Epoch: 9001 Train Accuracy: 0.7892 Validation Accuracy: 0.2143 Loss: 1.20704353 time: 0.0030s\n",
      "Epoch: 10001 Train Accuracy: 0.7892 Validation Accuracy: 0.2143 Loss: 1.20704353 time: 0.0030s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 35.5561s\n",
      "Epoch: 0001 Train Accuracy: 0.7892 Validation Accuracy: 0.2143 Loss: 1.20704353 time: 0.0030s\n"
     ]
    }
   ],
   "source": [
    "# set Train %\n",
    "\n",
    "train_percentage = .8\n",
    "    \n",
    "# Train set\n",
    "number_of_rows = features[0].shape[0]\n",
    "random_indices = np.random.choice(number_of_rows, size=int(train_percentage*number_of_rows), replace=False)\n",
    "val_indices = np.setdiff1d(np.arange(adj.shape[1]),random_indices)\n",
    "\n",
    "# Start Train\n",
    "prev_loss, op, val_op = train(adj,features,labels, random_indices, val_indices, val_features, True)\n",
    "\n",
    "# Keep training recurrently until the loss stops decreasing\n",
    "loss, op, val_op = train(adj,op.cpu().detach().numpy(),labels, random_indices, val_indices, val_op.cpu().detach().numpy())\n",
    "while loss < prev_loss :\n",
    "    prev_loss = loss\n",
    "    loss, op, val_op = train(adj,op.cpu().detach().numpy(),labels, random_indices, val_indices, val_op.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Basics can be found here:\n",
    "\n",
    "#### Introduction to Pytorch Tensors : https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "#### Calculating gradients using Autograd : https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html\n",
    "#### Building Pytorch Models : https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html\n",
    "#### Training Pytorch Models : https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
