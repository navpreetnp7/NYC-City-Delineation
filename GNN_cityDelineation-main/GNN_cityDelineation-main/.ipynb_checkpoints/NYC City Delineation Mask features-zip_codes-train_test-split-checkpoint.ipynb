{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City district delineation using Graph Neural Networks\n",
    "\n",
    "### Graph Neural Networks\n",
    "\n",
    "\n",
    "Graph Neural Networks (GNNs) are a class of deep learning methods designed to perform inference on data described by graphs. GNNs are neural networks that can be directly applied to graphs, and provide an easy way to do node-level, edge-level, and graph-level prediction tasks. \n",
    "\n",
    "Anything that is composed of linked entities can be represented as a graph. Graphs are excellent tools to visualize relations between people, objects, and concepts. With graphs becoming more pervasive and richer with information, and artificial neural networks becoming more popular and capable, GNNs have become a powerful tool for many important applications.\n",
    "\n",
    "<img src='GNN.png' width=\"600\" height=\"600\">\n",
    "\n",
    "Graph neural networks can be created like any other neural network, using fully connected layers, convolutional layers, pooling layers, etc. The type and number of layers depend on the type and complexity of the graph data and the desired output.\n",
    "\n",
    "The GNN receives the formatted graph data as input and produces a vector of numerical values that represent relevant information about nodes and their relations. This vector representation is called “graph embedding.”\n",
    "\n",
    "One very popular GNN architecture is the graph convolutional neural network (GCN), which uses convolution layers to create graph embeddings. (Kipf, T.N. and Welling, M., 2016. Semi-supervised classification with graph convolutional networks. https://arxiv.org/pdf/1609.02907.pdf)\n",
    "\n",
    "In the recent years, a lot of work has been done on the problem of generalizing neural networks to work on arbitrarily structured graphs - Bruna, J., Zaremba, W., Szlam, A. and LeCun, Y., 2013. Spectral networks and locally connected networks on graphs. https://arxiv.org/pdf/1312.6203.pdf%20http://arxiv.org/abs/1312.6203.pdf,  Henaff, M., Bruna, J. and LeCun, Y., 2015. Deep convolutional networks on graph-structured data. https://arxiv.org/abs/1506.05163 etc.\n",
    "\n",
    " \n",
    "Few applications for graph neural networks:\n",
    "\n",
    "- Node classification: One of the powerful applications of GNNs is adding new information to nodes or filling gaps where information is missing. For example, say you are running a social network and you have spotted a few bot accounts. Now you want to find out if there are other bot accounts in your network. You can train a GNN to classify other users in the social network as “bot” or “not bot” based on how close their graph embeddings are to those of the known bots.\n",
    "\n",
    "- Edge prediction: Another way to put GNNs to use is to find new edges that can add value to the graph. Going back to our social network, a GNN can find users (nodes) who are close to you in embedding space but who aren’t your friends yet (i.e., there isn’t an edge connecting you to each other). These users can then be introduced to you as friend suggestions.\n",
    "\n",
    "- Clustering: GNNs can glean new structural information from graphs. For example, in a social network where everyone is in one way or another related to others (through friends, or friends of friends, etc.), the GNN can find nodes that form clusters in the embedding space.\n",
    "\n",
    "A comprehensive tutorial on GNNs by Stanford is available on Youtube: https://www.youtube.com/watch?v=JAB_plj2rbA&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn\n",
    "\n",
    "### GNN for city delineation\n",
    "\n",
    "Here, we demonstrate an application for GNN for city borough delineation. The relationships among various entities such as interaction among people, intra-city mobility, social media interactions can be interpreted in terms of graphs with many features associated with nodes, edges. \n",
    "\n",
    "We will use the LEHD mobility network among zip codes in NYC and use it to learn the corresponding borough of zip codes (nodes) in the city. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data overview\n",
    "\n",
    "The LEHD mobility matrix contains the commute numbers between the home and work zip codes of the population..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('zips_merged.csv', delimiter=',')\n",
    "data = data.rename(columns={'total': 'weight', 'w_zip':'origin', 'h_zip':'destination'})\n",
    "data = data[data.destination.isin(data.origin.unique())]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'weight': commute flow between origin and destination zips\n",
    "- 'initialFeat': inital borough attachment labels for each zip code, assigned to be the borough with the closest distance between borough and zip centroids\n",
    "- 'true_label': true borough label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial model config\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "weight_decay = 10e-4\n",
    "epochs = 10001\n",
    "seed = 165\n",
    "hidden = 10\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a symmetric normalization for the propogating the layer, i.e. $$D^{−1/2}AD^{−1/2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(adj):\n",
    "\n",
    "    adj = torch.FloatTensor(adj)\n",
    "    adj_id = torch.FloatTensor(torch.eye(adj.shape[1]))\n",
    "    adj_id = adj_id.reshape((1, adj.shape[1], adj.shape[1]))\n",
    "    adj_id = adj_id.repeat(adj.shape[0], 1, 1)\n",
    "    adj = adj + adj_id\n",
    "    rowsum = torch.FloatTensor(adj.sum(2))\n",
    "    degree_mat_inv_sqrt = torch.diag_embed(torch.float_power(rowsum,-0.5), dim1=-2, dim2=-1).float()\n",
    "    adj_norm = torch.bmm(torch.transpose(torch.bmm(adj,degree_mat_inv_sqrt),1,2),degree_mat_inv_sqrt)\n",
    "\n",
    "    return adj_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function for the GNN is the double ReLU which is linear between between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doublerelu(x):\n",
    "    return torch.clamp(x, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Layer propogation rule of the GNN is : $$ H^{(l+1)} = f(H^{(l)}, A) = \\sigma ( H^{(l)}W_1^{(l)} + \\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W_2^{(l)}) $$\n",
    "\n",
    "where H is the l'th neural network layer, A is the adjaceny matrix, D is the diagonal node degree matrix, W1 , W2 are learnable weight matrices initialised as W1 = 1, W2 = 0  and sigma is the activation function doublerelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN1Layer(Module):\n",
    "\n",
    "    def __init__(self, batch_size, in_features, out_features):\n",
    "        super(GNN1Layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Initialse W1 = 1, W2 = 0 as pytorch learnable weights (parameters) that have require_grad = True which is\n",
    "        # required for calculating gradients while backpropogating using gradient descent\n",
    "        weight1_eye = torch.FloatTensor(torch.eye(in_features, out_features))\n",
    "        weight1_eye = weight1_eye.reshape((1, in_features, out_features))\n",
    "        weight1_eye = weight1_eye.repeat(batch_size, 1, 1)\n",
    "        self.weight1 = Parameter(weight1_eye)\n",
    "        self.weight2 = Parameter(torch.zeros(batch_size, in_features, out_features))\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # first term H*W1\n",
    "        v1 = torch.bmm(input, self.weight1)\n",
    "        # second term adj_norm*H*W2\n",
    "        v2 = torch.bmm(torch.bmm(adj, input), self.weight2)\n",
    "        # adding the two terms\n",
    "        output = v1 + v2\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN1(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, nfeat, ndim, hidden):\n",
    "        super(GNN1, self).__init__()\n",
    "\n",
    "        self.gc1 = GNN1Layer(batch_size, ndim, ndim)\n",
    "\n",
    "    def forward(self, x, adj, random_indices):\n",
    "        f = torch.clone(x)\n",
    "        # Applying activation function sigma (doublerelu) on the layer propogation\n",
    "        x = doublerelu(self.gc1(x, adj))\n",
    "        x = x/x.sum(axis=2).unsqueeze(2) #normalize st sum = 1\n",
    "        \n",
    "        # Only the masked nodes are updated in backpropogation \n",
    "        f[0][random_indices, :] = x[0][random_indices, :]\n",
    "        \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(adj,features,labels,random_indices,test_random_indices, test_features):\n",
    "    \n",
    "    # calculate symmetric normalisation for layer propogation\n",
    "    adj_norm = normalize(adj)\n",
    "    \n",
    "    labels = labels - 1\n",
    "    \n",
    "    # Convert from numpy to torch tensors\n",
    "    adj = torch.FloatTensor(adj)\n",
    "    adj_norm = torch.FloatTensor(adj_norm)\n",
    "    features = torch.FloatTensor(features)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "    test_features = torch.FloatTensor(test_features)\n",
    "    \n",
    "    # initialise the mode\n",
    "    model = GNN1(batch_size=adj.shape[0],\n",
    "                nfeat=adj.shape[1],\n",
    "                ndim=nb_label,\n",
    "                hidden=hidden)\n",
    "    # Transfer the weights to GPU for training\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "        features = features.cuda()\n",
    "        adj = adj.cuda()\n",
    "        adj_norm = adj_norm.cuda()\n",
    "        labels = labels.cuda()\n",
    "        test_features = test_features.cuda()\n",
    "    \n",
    "    # Train model\n",
    "    t_total = time.time()\n",
    "\n",
    "    # Using adam optimizers for backpropogation\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # loss function criteria\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train for the no of epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        # Pytorch accumulates gradient after every operation on tensors (defined by the model architecture)\n",
    "        # with require_grad = True. With each new epoch, we need to reset this gradient to 0 to calculate gradient\n",
    "        # for this epoch.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the output from forward propogation of our model\n",
    "        output = model(features, adj_norm, random_indices)\n",
    "            \n",
    "        # Mask accuracy\n",
    "        accuracy = torch.sum(torch.argmax(output[0][random_indices, :],axis=1)==labels[random_indices, :].reshape(1,-1))/labels[random_indices, :].shape[0]\n",
    "        \n",
    "        #Test accuracy.\n",
    "        # Use torch.no_grad() because test set is not used to calculate and update weights using the gradient value.\n",
    "        # No gradients will be calculated within this statement and hence no weights will be updated.\n",
    "        with torch.no_grad():\n",
    "            test_output = model(test_features, adj_norm, test_random_indices)\n",
    "            test_accuracy = torch.sum(torch.argmax(test_output[0][test_random_indices, :],axis=1)==labels[test_random_indices, :].reshape(1,-1))/labels[test_random_indices, :].shape[0]\n",
    "        \n",
    "        # Calculate the loss between our models training output and true label\n",
    "        loss = criterion(output[0],labels.reshape(-1).long())\n",
    "\n",
    "        # Calculate the gradients \n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        # Update the wieghts\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print summary of training \n",
    "        if epoch == 0:\n",
    "            best_loss = loss\n",
    "            best_output = output\n",
    "            best_acc = accuracy\n",
    "            init_acc = accuracy\n",
    "            test_init_acc = accuracy\n",
    "            best_test_acc = test_accuracy\n",
    "            best_test_op = test_output\n",
    "        else:\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_output = output\n",
    "                best_acc = accuracy\n",
    "                best_test_acc = test_accuracy\n",
    "                best_test_op = test_output\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch: {:04d}'.format(epoch + 1),\n",
    "                  'Train Accuracy: {:.4f}'.format(best_acc.item()),\n",
    "                  'Test Accuracy: {:.4f}'.format(best_test_acc.item()),\n",
    "                  'Loss: {:.8f}'.format(best_loss.item()),\n",
    "                  'time: {:.4f}s'.format(time.time() - t))\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    \n",
    "    return best_loss,best_output, init_acc, best_acc, best_test_op, best_test_acc, test_init_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading data, returns adjacency matrix, initial feature assignments and true labels\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    data = pd.read_csv('zips_merged.csv', delimiter=',')\n",
    "    data = data.rename(columns={'total': 'weight', 'w_zip':'origin', 'h_zip':'destination'})\n",
    "    data = data[data.destination.isin(data.origin.unique())]\n",
    "    G = nx.from_pandas_edgelist(data, 'origin', 'destination', 'weight',create_using=nx.DiGraph())\n",
    "    adj_list = np.array([nx.adjacency_matrix(G).todense()], dtype=float)\n",
    "    init_feat = np.array(data.groupby('origin')['initialFeat'].agg(['unique']))\n",
    "    true_label = np.array(data.groupby('origin')['true_label'].agg(['unique']))\n",
    "    init_feat = np.array(list(map(lambda x: x[0], init_feat))).reshape(-1, 1)\n",
    "    true_label = np.array(list(map(lambda x: x[0][0], true_label))).reshape(-1, 1)\n",
    "    return adj_list,init_feat,true_label\n",
    "\n",
    "adj,feature,labels = load_data()\n",
    "\n",
    "label = np.copy(labels)\n",
    "label = label - 1\n",
    "nb_label = int(max(label))+1\n",
    "featuress = np.eye(nb_label)[np.array(label,dtype=int).reshape(1,-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set mask %\n",
    "mask_percentage = [0.3]\n",
    "init = []\n",
    "final = []\n",
    "test_init = []\n",
    "test_final = []\n",
    "\n",
    "for m in mask_percentage:\n",
    "    \n",
    "    features = np.copy(featuress)\n",
    "    test_features = np.copy(featuress)\n",
    "    \n",
    "    # Masking for train and test set\n",
    "    number_of_rows = features[0].shape[0]\n",
    "    random_indices = np.random.choice(number_of_rows, size=int(m*number_of_rows), replace=False)\n",
    "    test_random_indices = np.random.choice(number_of_rows, size=int(0.1*number_of_rows), replace=False)\n",
    "    \n",
    "    # exclude low activity areas from masking\n",
    "    zip_sum = data.groupby(by='origin', as_index=False).sum()\n",
    "    low_act = zip_sum[zip_sum.weight < 3000].index\n",
    "    \n",
    "    random_indices = np.setdiff1d(random_indices, low_act)\n",
    "    random_rows = features[0][random_indices, :]\n",
    "    features[0][random_indices, :] = np.tile(np.array([[0.2]]),random_rows.shape)\n",
    "    \n",
    "    test_random_indices = np.setdiff1d(test_random_indices, low_act)\n",
    "    test_random_rows = test_features[0][test_random_indices, :]\n",
    "    test_features[0][test_random_indices, :] = np.tile(np.array([[0.2]]),test_random_rows.shape)\n",
    "    \n",
    "    \n",
    "    print(\"\\nMasked {}% of nodes\\n\".format(int(m*100)))\n",
    "    prev_loss, op, acc, _, test_op, _, test_acc = train(adj,features,labels, random_indices, test_random_indices, test_features)\n",
    "    init.append(acc.item())\n",
    "    test_init.append(test_acc.item())\n",
    "    #print(op)\n",
    "    loss, op, _, acc, test_op, test_acc, _ = train(adj,op.cpu().detach().numpy(),labels, random_indices, test_random_indices, test_op.cpu().detach().numpy())\n",
    "    i = 0\n",
    "    while loss < prev_loss :\n",
    "        i += 1\n",
    "        if i >= 2:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "        loss, op, _, acc, test_op, test_acc, _ = train(adj,op.cpu().detach().numpy(),labels, random_indices, test_random_indices, test_op.cpu().detach().numpy())\n",
    "    final.append(acc.item())\n",
    "    test_final.append(test_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Mask Percentage': [\"30/10\"], 'Train Initial Accuracy': init, 'Train Final Accuracy': final, 'Test Initial Accuracy': test_init, 'Test Final Accuracy': test_final}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "\n",
    "# load zip code shapefile\n",
    "zips = gpd.read_file('/Users/devashishkhulbe/Desktop/teaching/Labs_and_solutions/Data/zip_code/ZIP_CODE_040114.shp')\n",
    "zips['ZIPCODE'] = pd.to_numeric(zips['ZIPCODE'])\n",
    "zips = zips.to_crs(epsg=4326)\n",
    "\n",
    "# get unique zipcodes from LEHD data and merge with above shapefile\n",
    "zipsAll = data.groupby(by='origin', as_index=False).sum()\n",
    "zipsAll = zipsAll.merge(zips, left_on='origin', right_on='ZIPCODE')\n",
    "zipsAll.drop_duplicates(subset=['origin'], inplace=True)\n",
    "zipsAll.reset_index(drop=True, inplace=True)\n",
    "zipsAll = gpd.GeoDataFrame(zipsAll, crs=\"EPSG:4326\", geometry='geometry')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(20,12))\n",
    "\n",
    "# first plot\n",
    "# borough shapefile\n",
    "boro = gpd.read_file('/Users/devashishkhulbe/Desktop/research/HGNN/Borough Boundaries.zip')\n",
    "boro.plot(figsize=(10,10), ax=ax[0])\n",
    "\n",
    "colors = ['r', 'g', 'b', 'orange', 'lightgreen']\n",
    "\n",
    "# plot boroughs\n",
    "for ind, b in enumerate(boro.boro_code):\n",
    "    boro[boro.boro_code == b].plot(color=colors[ind], edgecolor=\"black\", ax=ax[0])\n",
    "    \n",
    "# plot masked zips\n",
    "zipsMasked = zipsAll[zipsAll.index.isin(random_indices)][['origin', 'geometry']]\n",
    "\n",
    "zipsMasked.plot(facecolor='lightgrey',hatch='///', edgecolor=\"black\", linestyle='--', ax=ax[0], label='masked zipcodes')\n",
    "    \n",
    "LegendElement = [mpatches.Patch(facecolor='lightgrey', hatch='//', label='masked zipcodes')]\n",
    "ax[0].axis('off')\n",
    "ax[0].legend(handles = LegendElement, loc='upper left')\n",
    "ax[0].set_title('borough delineations with masked zipcodes')\n",
    "\n",
    "\n",
    "# second plot\n",
    "# plot boroughs\n",
    "boro.plot(figsize=(10,10), ax=ax[1])\n",
    "\n",
    "colors = ['r', 'g', 'b', 'orange', 'lightgreen']\n",
    "# colors = ['g', 'r', 'orange', 'lightgreen', 'b']\n",
    "\n",
    "for ind, b in enumerate(boro.boro_code):\n",
    "    boro[boro.boro_code == b].plot(color=colors[ind], edgecolor=\"black\", ax=ax[1])\n",
    "    \n",
    "zipsMasked = zipsAll[zipsAll.index.isin(random_indices)][['origin', 'geometry']]\n",
    "\n",
    "# get predictions for masked zips\n",
    "pred = torch.argmax(op[0], dim=1).numpy()+1\n",
    "zipsMasked['pred'] = pred[np.sort(random_indices)]\n",
    "\n",
    "# plot masked zips\n",
    "for p in zipsMasked.pred.unique():\n",
    "    zipsMasked[zipsMasked.pred == p].plot(color=colors[p], edgecolor=\"black\", linestyle='--', ax=ax[1])\n",
    "    \n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(train_indices, test_indices):\n",
    "    # visualization\n",
    "\n",
    "    # load zip code shapefile\n",
    "    zips = gpd.read_file('ZIP_CODE_040114.shp')\n",
    "    zips['ZIPCODE'] = pd.to_numeric(zips['ZIPCODE'])\n",
    "    zips = zips.to_crs(epsg=4326)\n",
    "\n",
    "    # get unique zipcodes from LEHD data and merge with above shapefile\n",
    "    zipsAll = data.groupby(by='origin', as_index=False).sum()\n",
    "    zipsAll = zipsAll.merge(zips, left_on='origin', right_on='ZIPCODE')\n",
    "    zipsAll.drop_duplicates(subset=['origin'], inplace=True)\n",
    "    zipsAll.reset_index(drop=True, inplace=True)\n",
    "    zipsAll = gpd.GeoDataFrame(zipsAll, crs=\"EPSG:4326\", geometry='geometry')\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(20,12))\n",
    "\n",
    "    # first plot\n",
    "    # borough shapefile\n",
    "    boro = gpd.read_file('Borough Boundaries.zip')\n",
    "    boro.plot(figsize=(10,10), ax=ax[0])\n",
    "\n",
    "    colors = ['r', 'g', 'b', 'orange', 'lightgreen']\n",
    "    \n",
    "    # plot boroughs\n",
    "    for ind, b in enumerate(boro.boro_code):\n",
    "        boro[boro.boro_code == b].plot(color=colors[ind], edgecolor=\"black\", ax=ax[0])\n",
    "        \n",
    "    # plot masked zips - train set\n",
    "    zipsMasked_train = zipsAll[zipsAll.index.isin(train_indices)][['origin', 'geometry']]\n",
    "\n",
    "    zipsMasked_train.plot(facecolor='lightgrey',hatch='///', edgecolor=\"white\", linestyle='--',\n",
    "                          ax=ax[0], label='masked zipcodes (train)')\n",
    "    \n",
    "    # plot masked zips - test\n",
    "    zipsMasked_test = zipsAll[zipsAll.index.isin(test_indices)][['origin', 'geometry']]\n",
    "\n",
    "    zipsMasked_test.plot(facecolor='lightgrey',hatch='///', edgecolor=\"black\", linestyle='--',\n",
    "                         ax=ax[0], label='masked zipcodes (test)')\n",
    "\n",
    "    LegendElement = [mpatches.Patch(facecolor='lightgrey', hatch='//', \n",
    "                                    edgecolor=\"black\", linestyle='--', label='masked zipcodes (test)'), \n",
    "                    mpatches.Patch(facecolor='lightgrey', hatch='//', \n",
    "                                    edgecolor=\"white\", linestyle='--', label='masked zipcodes (train)')]\n",
    "    ax[0].axis('off')\n",
    "    ax[0].legend(handles = LegendElement, loc='upper left')\n",
    "    ax[0].set_title('borough delineations with masked zipcodes')\n",
    "\n",
    "    # second plot\n",
    "    # plot boroughs\n",
    "    boro.plot(figsize=(10,10), ax=ax[1])\n",
    "\n",
    "    colors = ['r', 'g', 'b', 'orange', 'lightgreen']\n",
    "\n",
    "    for ind, b in enumerate(boro.boro_code):\n",
    "        boro[boro.boro_code == b].plot(color=colors[ind], edgecolor=\"black\", ax=ax[1])\n",
    "        \n",
    "    # get predictions for masked zips\n",
    "    zipsMasked_predTest = zipsAll[zipsAll.index.isin(test_indices)][['origin', 'geometry']]\n",
    "\n",
    "    pred = torch.argmax(op[0], dim=1).numpy()+1\n",
    "    zipsMasked_predTest['pred'] = pred[np.sort(test_indices)]\n",
    "\n",
    "    # plot masked zips\n",
    "    for p in zipsMasked_predTest.pred.unique():\n",
    "        zipsMasked_predTest[zipsMasked_predTest.pred == p].plot(color=colors[p], edgecolor=\"black\", \n",
    "                                                                linestyle='--', ax=ax[1])\n",
    "\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(random_indices, test_random_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Basics can be found here:\n",
    "\n",
    "#### Introduction to Pytorch Tensors : https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "#### Calculating gradients using Autograd : https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html\n",
    "#### Building Pytorch Models : https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html\n",
    "#### Training Pytorch Models : https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
